# Lane_Detection_For_Autonomous_Car
Computer Vision Project-2
Pipeline explanation: - 
 
1. First, the image is undistorted using the calibration matrix and distortion vector provided as a supplement for the project. 
2. Extract four points around the lanes (the current lane) for performing the homography operation. Perform the homography operation using the inbuilt “homography” function or the “getPerspectiveTransform” function. The acquired homography region is previewed on the screen using the inbuilt function “warpPerpective”. 
3. In the homography image operations like brightness and contrast are increased using add and multiply functions respectively on the frame. Brightness and contrast are increased uniformly across the frame. Only brightness is increased for the white lane. Brightness is increased twice and contrast is increased once for the yellow lane. Parameters like “alpha” and “beta” variables are used for these changes. This is done to avoid mismatch, in different conditions like bright daylight, gloomy day, rainy day or shadow region as in tunnels or due to side plantations. 
4. After getting the homography image,all the color calibrations are done. Masking of yellow color is done using the HSV color space and masking of white color is done using the BGR color space. For masking the variables like “lower_white”, “upper_white”, “lower_yellow” and  “upper_yellow” are used to specify the upper and lower bounds of the colors in their respective color space(HSV for yellow and BGR for white). 
5. After the masking, the Shi-Tomasi method of detecting corners is used, the function “goodFeaturesToTrack” is used on the two different masking images. Shi-Tomasi method is used to determine the corners in the yellow and white lanes respectively. 
6. The corners, if detected in between the lanes are rejected. So as to choose the best-required points/regions (here the yellow and white regions in our current lane). 
7. Once the required yellow and the white pixels are found using Shi-Tomasi we use the function named “polyfit” to fit a line through those pixels(corners). 
8. Now using the inverse homography to find points in which the line has been to be drawn in the video, we project the lane lines on the actual frame of the video. After observing the stability of the lines fitted on the main video frame we shade the region enclosed within the lanes using the function “fillPoly”. If the stability is not up to mark we tune the parameters mentioned above to see if the stability can be increased, which we have successfully accomplished. 
9. For turn detection, we compared the distance between the middle of the yellow line and white line with the middle of the frame in the homography image i.e. 100. If the mid-point is before 100 then the car is turning left, similarly, if the mid-point is beyond 100 then the car is moving right. If the car moves straight then the mid-point is near 100 theoretically. 
10. As our homography is not perfect, we have decided to give a range in which the car moves straight. If the midpoint is before 95 then the car is turning left, if mid-point is beyond 107 then the car is turning right. If the mid-point is in between 95 and 107 then the car is moving straight.  
11. After this we test this code in different scenarios especially in the 2 videos provided in the assignment. There is a minor shift of the lane region in the left side under the bridge which is kind of acceptable because if we analyze each frame under the bridge in first 3-4 frames the white lines are not at all visible. And there is minor gitterning after the car comes out of the bridge as the change in intensity of light is very high. We believe this issue can be solved with superior algorithms and hardware. 
